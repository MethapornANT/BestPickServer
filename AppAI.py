from transformers import AutoImageProcessor, SiglipForImageClassification
from apscheduler.schedulers.background import BackgroundScheduler
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sqlalchemy import Enum as SAEnum
from sqlalchemy.sql import text
from flask_sqlalchemy import SQLAlchemy
from sklearn.neighbors import NearestNeighbors
from jwt.exceptions import ExpiredSignatureError, InvalidTokenError
from torchvision import models, transforms
from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename
from selenium import webdriver
from surprise import SVD, Dataset, Reader
from qrcode.constants import ERROR_CORRECT_H
from textblob import TextBlob
from functools import wraps
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from sqlalchemy import create_engine
from pythainlp.tokenize import word_tokenize
from PIL import Image, ImageOps, ImageFile
from datetime import datetime, date, timedelta, timezone
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import traceback
import requests
import threading
import seaborn as sns
import joblib
import locale
import random
import pickle
import base64
import qrcode
import secrets
import uuid
import torch
import torch.nn as nn
import time
import json
import pytz
import re
import io
import sys
import os

import jwt
from jwt.exceptions import ExpiredSignatureError, InvalidTokenError


# optional deps / locale
try:
    from promptpay import qrcode as promptpay_qrcode
except ImportError:
    promptpay_qrcode = None

try:
    locale.setlocale(locale.LC_ALL, 'th_TH.UTF-8')
except locale.Error:
    try:
        locale.setlocale(locale.LC_ALL, 'thai')
    except locale.Error:
        print("‚ö†Ô∏è [WARN] Could not set Thai locale. Date formatting might not show full Thai month names.")

app = Flask(__name__)

# ==================== NSFW DETECTION SETUP ====================

# ‡∏Å‡∏±‡∏ô‡∏£‡∏π‡∏õ‡∏¢‡∏±‡∏Å‡∏©‡πå/‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≤‡∏î‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡πÇ‡∏õ‡∏£‡πÄ‡∏ã‡∏™‡πÄ‡∏î‡πâ‡∏á (‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏î‡∏¥‡∏°)
Image.MAX_IMAGE_PIXELS = 25_000_000
ImageFile.LOAD_TRUNCATED_IMAGES = True

# === Config: upload path & model path ===
UPLOAD_FOLDER = './uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

MODEL_PATH = './NSFW_Model_archived_20250803-132149/results/20250801-100650 Best Model/efficientnetb0/models/efficientnetb0_best.pth'

# === Threshold & device (‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á print ‡πÅ‡∏•‡∏∞ return) ===
HENTAI_THRESHOLD = 25.0
PORN_THRESHOLD = 20.0

# === Model: Custom EfficientNetB0 (‡πÇ‡∏Ñ‡∏£‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô) ===
class CustomEfficientNetB0(nn.Module):
    def __init__(self, num_classes=5):
        super(CustomEfficientNetB0, self).__init__()
        self.efficientnet = models.efficientnet_b0(pretrained=False)
        self.efficientnet.classifier[1] = nn.Linear(self.efficientnet.classifier[1].in_features, num_classes)

    def forward(self, x):
        return self.efficientnet(x)

# === Labels: mapping id <-> label ===
LABELS = ['normal', 'hentai', 'porn', 'sexy', 'anime']
label2idx = {label: idx for idx, label in enumerate(LABELS)}
idx2label = {idx: label for label, idx in label2idx.items()}

# === Load model: ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡πà‡∏≠‡∏ô‡πÇ‡∏¢‡∏ô ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏õ‡∏£‡πÄ‡∏ã‡∏™‡∏ï‡∏≤‡∏¢ ===
model = CustomEfficientNetB0(num_classes=len(LABELS))
MODEL_READY = True
try:
    state_dict_from_file = torch.load(MODEL_PATH, map_location=torch.device('cpu'))
    try:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå .pth ‡πÄ‡∏ã‡∏ü‡∏°‡∏≤‡∏à‡∏≤‡∏Å backbone ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        model.efficientnet.load_state_dict(state_dict_from_file)
    except Exception:
        # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ key ‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πà‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
        model.load_state_dict(state_dict_from_file, strict=False)
    print("‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
except Exception as e:
    # ‡πÑ‡∏°‡πà exit; ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ (‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™)
    print(f"‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå: {e}")
    MODEL_READY = False

model.eval()

# === Preprocess: ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô + ‡πÅ‡∏Å‡πâ EXIF orientation ===
processor = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# === Inference API (file path): ‡∏ï‡∏£‡∏ß‡∏à NSFW ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå ===
def nude_predict_image(image_path):
    try:
        # ‡πÄ‡∏õ‡∏¥‡∏î‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡πÅ‡∏Å‡πâ‡πÄ‡∏≠‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å EXIF + ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö RGB
        image = ImageOps.exif_transpose(Image.open(image_path)).convert("RGB")
        inputs = processor(image).unsqueeze(0)

        # ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á device ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô CPU ‡∏Å‡πá‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô CPU)
        device = next(model.parameters()).device
        inputs = inputs.to(device)

        with torch.inference_mode():
            if MODEL_READY:
                outputs = model(inputs)
                logits = outputs
                probs = torch.softmax(logits, dim=1).squeeze().tolist()
            else:
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏±‡∏á
                probs = [0.0] * len(LABELS)

        # ‡∏Å‡∏±‡∏ô index ‡∏û‡∏±‡∏á (‡∏õ‡∏Å‡∏ï‡∏¥‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏ñ‡πâ‡∏≤ LABELS = 5 ‡∏Ñ‡∏•‡∏≤‡∏™)
        if label2idx['hentai'] >= len(probs) or label2idx['porn'] >= len(probs):
            raise IndexError("‡πÑ‡∏°‡πà‡∏û‡∏ö Index ‡∏Ç‡∏≠‡∏á 'hentai' ‡∏´‡∏£‡∏∑‡∏≠ 'porn' ‡πÉ‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå")

        hentai_score = probs[label2idx['hentai']] * 100
        porn_score = probs[label2idx['porn']] * 100

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏ô log/console ‡πÉ‡∏´‡πâ dev ‡∏ï‡∏≤‡∏°‡∏î‡∏π (‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö)
        is_nsfw = (hentai_score >= HENTAI_THRESHOLD) or (porn_score >= PORN_THRESHOLD)
        print(f"NSFW Detection for {image_path}:")
        print(f"   Hentai: {hentai_score:.2f}%")
        print(f"   Pornography: {porn_score:.2f}%")
        print(f"   Is NSFW: {is_nsfw} (thresholds: hentai>={HENTAI_THRESHOLD} | porn>={PORN_THRESHOLD})")

        # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå
        result_dict = {}
        for i in range(len(probs)):
            if i in idx2label:
                result_dict[idx2label[i]] = round(probs[i]*100, 2)
            else:
                result_dict[f"Class_{i}"] = round(probs[i]*100, 2)

        # ‡πÅ‡∏õ‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ degraded ‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ù‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ô‡∏µ‡πâ)
        if not MODEL_READY:
            result_dict["__degraded__"] = 1.0

        return is_nsfw, result_dict

    except Exception as e:
        # ‡∏à‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô error ‡πÉ‡∏ô payload ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ï‡∏≤‡∏¢
        print(f"Error in NSFW detection for {image_path}: {e}")
        return False, {"error": f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ: {e}"}


# ========== App/DB Config (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô URI ‡πÄ‡∏î‡∏¥‡∏°) ==========
# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ URI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
app.config['SQLALCHEMY_DATABASE_URI'] = 'mysql+mysqlconnector://root:1234@localhost/bestpick'

# ‡∏•‡∏î overhead ‡∏Ç‡∏≠‡∏á SQLAlchemy ‡πÅ‡∏•‡∏∞‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ô‡∏Ñ‡∏ä‡∏±‡∏ô‡πÄ‡∏ô‡πà‡∏≤
app.config.setdefault('SQLALCHEMY_TRACK_MODIFICATIONS', False)
app.config.setdefault('SQLALCHEMY_ENGINE_OPTIONS', {
    'pool_pre_ping': True,     # ping ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ô‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ï‡∏≤‡∏¢
    'pool_recycle': 1800,      # ‡∏£‡∏µ‡πÑ‡∏ã‡πÄ‡∏Ñ‡∏¥‡∏•‡∏ó‡∏∏‡∏Å 30 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏Å‡∏±‡∏ô MySQL ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ô‡∏Ñ‡∏ä‡∏±‡∏ô
    'pool_size': 10,           # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£
    'max_overflow': 20,
})

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô SQLAlchemy
db = SQLAlchemy(app)

# ========== SQLAlchemy Models: Order / Ad / AdPackage ==========
# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á/‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå/‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°

# ‡πÇ‡∏°‡πÄ‡∏î‡∏• Order (‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠)
class Order(db.Model):
    __tablename__ = 'orders'

    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, nullable=False)

    renew_ads_id = db.Column(db.Integer, nullable=True)
    package_id = db.Column(db.Integer, nullable=True)

    amount = db.Column(db.Numeric(10, 2), nullable=False)
    promptpay_qr_payload = db.Column(db.String(255), nullable=True)

    status = db.Column(
        SAEnum('pending', 'approved', 'paid', 'active', 'rejected', 'expired', name='order_status_enum'),
        nullable=False,
        default='pending'
    )

    slip_image = db.Column(db.String(255), nullable=True)

    created_at = db.Column(db.DateTime, default=datetime.now)  # ‡∏Ñ‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
    updated_at = db.Column(db.DateTime, default=datetime.now, onupdate=datetime.now)

    show_at = db.Column(db.Date, nullable=True)

    def __repr__(self):
        return f'<Order {self.id}>'


# ‡πÇ‡∏°‡πÄ‡∏î‡∏• Ad (‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤)
class Ad(db.Model):
    __tablename__ = 'ads'

    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, nullable=False)
    order_id = db.Column(db.Integer, nullable=False)

    title = db.Column(db.String(255), nullable=False)
    content = db.Column(db.Text, nullable=False)
    link = db.Column(db.String(255), nullable=True)
    image = db.Column(db.String(255), nullable=True)

    status = db.Column(
        SAEnum('pending', 'approved', 'paid', 'active', 'rejected', 'expired', name='ad_status_enum'),
        nullable=False,
        default='pending'
    )

    created_at = db.Column(db.DateTime, default=datetime.now)
    updated_at = db.Column(db.DateTime, default=datetime.now, onupdate=datetime.now)

    expiration_date = db.Column(db.Date, nullable=True)

    admin_notes = db.Column(db.Text, nullable=True)
    admin_slip = db.Column(db.String(255), nullable=True)

    show_at = db.Column(db.Date, nullable=True)

    def __repr__(self):
        return f'<Ad {self.id}>'


# ‡πÇ‡∏°‡πÄ‡∏î‡∏• AdPackage (‡πÅ‡∏û‡πá‡∏Å‡πÄ‡∏Å‡∏à‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤)
class AdPackage(db.Model):
    __tablename__ = 'ad_packages'

    package_id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    price = db.Column(db.Numeric(10, 2), nullable=False)
    duration_days = db.Column(db.Integer, nullable=False)

    def __repr__(self):
        return f'<AdPackage {self.package_id}>'


# ========== Secrets/Env ==========
load_dotenv()
JWT_SECRET = os.getenv('JWT_SECRET')  # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏ô .env
if not JWT_SECRET:
    # ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÅ‡∏≠‡∏õ‡∏•‡πà‡∏° (‡πÄ‡∏ú‡∏∑‡πà‡∏≠ dev ‡∏•‡∏∑‡∏°‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤)
    print("[WARN] JWT_SECRET is not set in environment. Please configure it for production.")



# ==================== RECOMMENDATION SYSTEM FUNCTIONS ====================



# === Global caches & params (‡∏Ñ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏¥‡∏°) ===
recommendation_cache = {}
impression_history_cache = {}

CACHE_EXPIRY_TIME_SECONDS = 120
IMPRESSION_HISTORY_TTL_SECONDS = 3600
IMPRESSION_HISTORY_MAX_ENTRIES = 100

# === ‡πÄ‡∏û‡∏¥‡πà‡∏° lock ‡∏Å‡∏±‡∏ô race condition ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á thread ‡∏Å‡∏±‡∏ö request ===
_cache_lock = threading.Lock()

# === Background cache janitor: ‡∏•‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ thread ‡∏ï‡∏≤‡∏¢ ===
def clear_cache():
    while True:
        try:
            now = datetime.now()
            with _cache_lock:
                # ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡πÅ‡∏Ñ‡∏ä‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏ö‡∏ö in-place ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ reference ‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏•‡∏∏‡∏î
                recommendation_cache.clear()

                # TTL prune ‡∏Ç‡∏≠‡∏á impression history ‡πÅ‡∏ö‡∏ö in-place
                to_delete = []
                for user_id, items in impression_history_cache.items():
                    pruned = [e for e in items if (now - e['timestamp']).total_seconds() < IMPRESSION_HISTORY_TTL_SECONDS]
                    if pruned:
                        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÑ‡∏ß‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô MAX_ENTRIES
                        impression_history_cache[user_id] = pruned[-IMPRESSION_HISTORY_MAX_ENTRIES:]
                    else:
                        to_delete.append(user_id)
                for uid in to_delete:
                    del impression_history_cache[uid]
        except Exception as e:
            # ‡∏Å‡∏±‡∏ô thread ‡∏ï‡∏≤‡∏¢‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ
            print(f"[CACHE JANITOR] error: {e}")
        finally:
            time.sleep(CACHE_EXPIRY_TIME_SECONDS)

# === Start daemon thread (‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô) ===
threading.Thread(target=clear_cache, daemon=True).start()

# === JWT verify: ‡∏ï‡∏£‡∏ß‡∏à token ‡πÅ‡∏•‡πâ‡∏ß‡∏ú‡∏π‡∏Å user_id/role ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö request ===
def verify_token(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        auth_header = request.headers.get("Authorization")
        if not auth_header or not auth_header.startswith("Bearer "):
            return jsonify({"error": "No token provided or incorrect format"}), 403
        token = auth_header.split(" ")[1]
        try:
            decoded = jwt.decode(token, JWT_SECRET, algorithms=["HS256"])
            request.user_id = decoded.get("id")
            request.role = decoded.get("role")
        except jwt.ExpiredSignatureError:
            return jsonify({"error": "Unauthorized: Token has expired"}), 401
        except jwt.InvalidTokenError:
            return jsonify({"error": "Unauthorized: Invalid token"}), 401
        return f(*args, **kwargs)
    return decorated_function

# === DB loader: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Content-based ‡πÅ‡∏•‡∏∞ Collaborative ‡∏à‡∏≤‡∏Å MySQL ‡∏î‡πâ‡∏ß‡∏¢ pool ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ===
def load_data_from_db():
    try:
        engine = create_engine(
            'mysql+mysqlconnector://root:1234@localhost/bestpick',
            pool_pre_ping=True, pool_recycle=1800, pool_size=5, max_overflow=10
        )
        content_based_data = pd.read_sql("SELECT * FROM contentbasedview;", con=engine)
        print("‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Content-Based ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        collaborative_data = pd.read_sql("SELECT * FROM collaborativeview;", con=engine)
        print("‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Collaborative ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        return content_based_data, collaborative_data
    except Exception as e:
        print(f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}")
        raise

# === Normalization utils: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡πÄ‡∏Å‡∏• 0..1 ===
def normalize_scores(series):
    min_val, max_val = series.min(), series.max()
    if max_val > min_val:
        return (series - min_val) / (max_val - min_val)
    return series

# === Engagement normalization: ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ ===
def normalize_engagement(data, user_column='owner_id', engagement_column='PostEngagement'):
    data = data.copy()
    data['NormalizedEngagement'] = data.groupby(user_column)[engagement_column].transform(lambda x: normalize_scores(x))
    return data

# === Comment analyzer: ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ; ===
def analyze_comments(comments_series):
    comment_counts = []
    for comment_text in comments_series:
        if pd.isna(comment_text) or str(comment_text).strip() == '':
            comment_counts.append(0)
        else:
            individual_comments = [c.strip() for c in str(comment_text).split(';') if c.strip()]
            comment_counts.append(len(individual_comments))
    return comment_counts

# === Content-based model: TF-IDF + KNN ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å engagement/comment ===
def create_content_based_model(data, text_column='Content', comment_column='Comments', engagement_column='PostEngagement'):
    required_columns = [text_column, comment_column, engagement_column]
    if not all(col in data.columns for col in required_columns):
        missing = set(required_columns) - set(data.columns)
        raise ValueError(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {missing}")

    data = data.copy()
    train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)

    tfidf = TfidfVectorizer(stop_words='english', max_features=6000, ngram_range=(1, 3), min_df=1, max_df=0.8)
    tfidf_matrix = tfidf.fit_transform(train_data[text_column].fillna(''))

    knn = NearestNeighbors(n_neighbors=10, metric='cosine')
    knn.fit(tfidf_matrix)

    train_data['CommentCount'] = analyze_comments(train_data[comment_column])
    test_data['CommentCount'] = analyze_comments(test_data[comment_column])

    max_comment_count = int(train_data['CommentCount'].max()) if len(train_data) else 0
    if max_comment_count > 0:
        train_data['NormalizedCommentCount'] = train_data['CommentCount'] / max_comment_count
        test_data['NormalizedCommentCount'] = test_data['CommentCount'] / max_comment_count
    else:
        train_data['NormalizedCommentCount'] = 0.0
        test_data['NormalizedCommentCount'] = 0.0

    train_data = normalize_engagement(train_data, engagement_column=engagement_column)
    train_data['NormalizedEngagement'] = normalize_scores(train_data[engagement_column])
    train_data['WeightedEngagement'] = train_data['NormalizedEngagement'] + train_data['NormalizedCommentCount']

    test_data = normalize_engagement(test_data, engagement_column=engagement_column)
    test_data['WeightedEngagement'] = test_data['NormalizedEngagement'] + test_data['NormalizedCommentCount']

    joblib.dump(tfidf, 'TFIDF_Model.pkl', compress=3)
    joblib.dump(knn, 'KNN_Model.pkl', compress=3)

    return tfidf, knn, train_data, test_data

# === Collaborative model (SVD): ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏° ===
def create_collaborative_model(data, n_factors=150, n_epochs=70, lr_all=0.005, reg_all=0.5):
    required_columns = ['user_id', 'post_id']
    if not all(col in data.columns for col in required_columns):
        missing = set(required_columns) - set(data.columns)
        raise ValueError(f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {missing}")

    melted_data = data.melt(id_vars=['user_id', 'post_id'], var_name='category', value_name='score')
    melted_data = melted_data[melted_data['score'] > 0]

    if melted_data.empty:
        # ‡∏Å‡∏±‡∏ô‡∏û‡∏±‡∏á: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏•‡∏¢ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÜ ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Å‡∏•‡∏≤‡∏á‡πÜ
        class DummySVD:
            def predict(self, uid, iid):
                class Est: est = 0.5
                return Est()
        model = DummySVD()
        joblib.dump(model, 'Collaborative_Model.pkl', compress=3)
        return model, melted_data

    train_data, test_data = train_test_split(melted_data, test_size=0.25, random_state=42)

    reader = Reader(rating_scale=(float(melted_data['score'].min()), float(melted_data['score'].max())))
    trainset = Dataset.load_from_df(train_data[['user_id', 'post_id', 'score']], reader).build_full_trainset()

    model = SVD(n_factors=n_factors, n_epochs=n_epochs, lr_all=lr_all, reg_all=reg_all)
    model.fit(trainset)

    joblib.dump(model, 'Collaborative_Model.pkl', compress=3)
    return model, test_data

# === Hybrid recommend: ‡∏£‡∏ß‡∏° collaborative + content + category ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ ===
def recommend_hybrid(user_id, all_posts_data, collaborative_model, knn, tfidf, categories, alpha=0.50, beta=0.20):
    if not (0 <= alpha <= 1):
        raise ValueError("Alpha ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0 ‡∏ñ‡∏∂‡∏á 1")
    if not (0 <= beta <= 1):
        raise ValueError("Beta ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0 ‡∏ñ‡∏∂‡∏á 1")

    recommendations = []
    has_knn = hasattr(knn, "_fit_X") and getattr(knn, "_fit_X") is not None and getattr(knn, "_fit_X").shape[0] > 0
    has_tfidf = hasattr(tfidf, "vocabulary_") and tfidf.vocabulary_ is not None

    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ normalized engagement ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡∏°‡∏¥‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô‡∏ñ‡∏≠‡∏¢‡πÑ‡∏õ 0
    norm_eng_col = 'NormalizedEngagement'
    if norm_eng_col not in all_posts_data.columns:
        all_posts_data = all_posts_data.copy()
        all_posts_data[norm_eng_col] = 0.0

    for _, post in all_posts_data.iterrows():
        post_id = post['post_id']

        # Collaborative
        collab_score = 0.5
        try:
            collab_score = collaborative_model.predict(user_id, post_id).est
        except Exception:
            pass

        # Content-based
        content_score = 0.0
        if has_tfidf and has_knn:
            try:
                post_content = str(post.get('Content', '')) if pd.notna(post.get('Content', '')) else ''
                tfidf_vector = tfidf.transform([post_content])
                n_neighbors = min(20, knn._fit_X.shape[0])
                distances, indices = knn.kneighbors(tfidf_vector, n_neighbors=n_neighbors)
                if len(indices[0]) > 0:
                    content_score = float(np.mean([all_posts_data.iloc[i][norm_eng_col] for i in indices[0]]))
            except Exception as e:
                print(f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πç‡∏≤‡∏ô‡∏ß‡∏ì Content-Based score ‡∏™‡πç‡∏≤‡∏´‡∏£‡∏±‡∏ö post_id {post_id}: {e}")

        # Category-based
        category_score = 0.0
        if categories:
            cnt = 0
            for category in categories:
                if category in post.index and pd.notna(post[category]) and post[category] == 1:
                    cnt += 1
            if len(categories) > 0:
                category_score = cnt / len(categories)

        # Final score
        final_score = (alpha * float(collab_score)) + ((1 - alpha) * float(content_score)) + (beta * float(category_score))
        recommendations.append((post_id, final_score))

    recommendations_df = pd.DataFrame(recommendations, columns=['post_id', 'score'])
    recommendations_df['random_order'] = np.random.rand(len(recommendations_df))

    if not recommendations_df['score'].empty and recommendations_df['score'].nunique() > 1:
        recommendations_df['normalized_score'] = normalize_scores(recommendations_df['score'])
    else:
        recommendations_df['normalized_score'] = 0.5

    return recommendations_df.sort_values(by=['normalized_score', 'random_order'], ascending=[False, False])['post_id'].tolist()

# === Split & rank: ‡∏à‡∏±‡∏î‡∏ä‡∏±‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏™‡∏î‡πÉ‡∏´‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô, ‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÇ‡∏ï‡πâ‡∏ï‡∏≠‡∏ö, ‡πÄ‡∏Ñ‡∏¢‡πÇ‡∏ï‡πâ‡∏ï‡∏≠‡∏ö ===
def split_and_rank_recommendations(recommendations, user_interactions, impression_history, total_posts_in_db):
    unique_recommendations_ids = [int(p) if isinstance(p, float) else p for p in list(dict.fromkeys(recommendations))]
    user_interactions_set = set([int(p) if isinstance(p, float) else p for p in user_interactions])
    impression_history_set = set([int(p) for entry in impression_history for p in [entry['post_id']]])

    final_recommendations_ordered = []

    truly_unviewed_posts = [
        post_id for post_id in unique_recommendations_ids
        if post_id not in user_interactions_set and post_id not in impression_history_set
    ]

    recently_shown_not_interacted = [
        post_id for post_id in unique_recommendations_ids
        if post_id not in user_interactions_set and post_id in impression_history_set
    ]

    interacted_posts = [
        post_id for post_id in unique_recommendations_ids
        if post_id in user_interactions_set
    ]

    num_fresh_priority = min(len(truly_unviewed_posts), max(10, int(len(unique_recommendations_ids) * 0.25)))
    final_recommendations_ordered.extend(truly_unviewed_posts[:num_fresh_priority])

    remaining_posts_to_mix = [
        post_id for post_id in unique_recommendations_ids
        if post_id not in set(final_recommendations_ordered)
    ]

    group_A_not_recently_shown = [
        post_id for post_id in remaining_posts_to_mix
        if post_id not in impression_history_set
    ]

    group_B_recently_shown = [
        post_id for post_id in remaining_posts_to_mix
        if post_id in impression_history_set
    ]

    num_to_demote_from_history = min(len(impression_history), int(len(impression_history) * 0.25))
    posts_to_demote = set([entry['post_id'] for entry in impression_history[:num_to_demote_from_history]])

    demoted_posts, non_demoted_posts = [], []
    for post_id in unique_recommendations_ids:
        (demoted_posts if post_id in posts_to_demote else non_demoted_posts).append(post_id)

    truly_unviewed_non_demoted = [
        post_id for post_id in non_demoted_posts
        if post_id not in user_interactions_set and post_id not in impression_history_set
    ]

    remaining_non_demoted_and_not_truly_unviewed = [
        post_id for post_id in non_demoted_posts
        if post_id not in set(truly_unviewed_non_demoted)
    ]

    num_unviewed_first_current_run = min(30, len(truly_unviewed_non_demoted))
    final_recommendations_ordered.extend(truly_unviewed_non_demoted[:num_unviewed_first_current_run])

    remaining_to_shuffle_and_mix = (
        truly_unviewed_non_demoted[num_unviewed_first_current_run:]
        + remaining_non_demoted_and_not_truly_unviewed
        + demoted_posts
    )

    shuffled_segment = []
    block_size = 5
    blocks = [remaining_to_shuffle_and_mix[i:i + block_size] for i in range(0, len(remaining_to_shuffle_and_mix), block_size)]

    for block in blocks:
        block_with_priority = []
        for post_id in block:
            priority_score = 2 if post_id in posts_to_demote else (1 if post_id in impression_history_set else 0)
            block_with_priority.append((post_id, priority_score))
        block_with_priority.sort(key=lambda x: (x[1], random.random()))
        shuffled_segment.extend([post_id for post_id, _ in block_with_priority])

    final_recommendations_ordered.extend(shuffled_segment)

    print("Top N Unviewed (prioritized):", final_recommendations_ordered[:num_unviewed_first_current_run])
    print("Remaining Overall Recommendations (block-shuffled, with recently shown and demoted pushed back):", final_recommendations_ordered[num_unviewed_first_current_run:])
    print("Final Recommendations (ordered, after mix):", final_recommendations_ordered)

    return final_recommendations_ordered


# ==================== SLIP & PROMPTPAY FUNCTIONS (from Slip.py) ====================

# === Order finder: ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏î‡πâ‡∏ß‡∏¢ ID (‡∏Ñ‡∏∑‡∏ô dict ‡∏´‡∏£‡∏∑‡∏≠ None) ===
def find_order_by_id(order_id):
    try:
        order = Order.query.filter_by(id=order_id).first()
        if not order:
            return None
        return {
            'id': order.id,
            'user_id': order.user_id,
            'amount': order.amount,  # ‡∏Ñ‡∏á type ‡πÄ‡∏î‡∏¥‡∏° (Numeric/Decimal) ‡πÑ‡∏°‡πà‡πÅ‡∏Å‡πâ‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°
            'status': order.status,
            'promptpay_qr_payload': order.promptpay_qr_payload,
            'slip_image': order.slip_image,
            'renew_ads_id': order.renew_ads_id,
            'package_id': order.package_id,
            'show_at': order.show_at
        }
    except Exception as e:
        print(f"‚ùå [ERROR] find_order_by_id({order_id}): {e}")
        return None

# === Ad finder by order_id: ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞/‡∏ß‡∏±‡∏ô‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡∏π‡∏Å‡∏Å‡∏±‡∏ö‡∏≠‡∏≠‡πÄ‡∏î‡∏≠‡∏£‡πå ===
def find_ad_by_order_id(order_id):
    try:
        ad = Ad.query.filter_by(order_id=order_id).first()
        if not ad:
            return None
        return {
            'id': ad.id,
            'status': ad.status,
            'expiration_date': ad.expiration_date,
            'show_at': ad.show_at
        }
    except Exception as e:
        print(f"‚ùå [ERROR] find_ad_by_order_id({order_id}): {e}")
        return None

# === Ad finder by ad_id: ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡πÄ‡∏ï‡πá‡∏°‡∏Å‡πâ‡∏≠‡∏ô ===
def find_ad_by_id(ad_id):
    try:
        ad = Ad.query.filter_by(id=ad_id).first()
        if not ad:
            return None
        return {
            'id': ad.id,
            'user_id': ad.user_id,
            'order_id': ad.order_id,
            'title': ad.title,
            'content': ad.content,
            'link': ad.link,
            'image': ad.image,
            'status': ad.status,
            'expiration_date': ad.expiration_date,
            'created_at': ad.created_at,
            'updated_at': ad.updated_at,
            'show_at': ad.show_at
        }
    except Exception as e:
        print(f"‚ùå [ERROR] find_ad_by_id({ad_id}): {e}")
        return None

# === Package duration getter: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡∏±‡∏ô‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡∏à‡∏≤‡∏Å‡πÅ‡∏û‡πá‡∏Å‡πÄ‡∏Å‡∏à ===
def get_ad_package_duration(package_id):
    try:
        pkg = AdPackage.query.filter_by(package_id=package_id).first()
        if not pkg:
            print(f"‚ùå [ERROR] AdPackage with ID {package_id} not found.")
            return None
        return pkg.duration_days
    except Exception as e:
        print(f"‚ùå [ERROR] get_ad_package_duration({package_id}): {e}")
        return None

# === Order status/slip updater: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ + ‡πÄ‡∏Å‡πá‡∏ö path ‡∏™‡∏•‡∏¥‡∏õ (‡∏ó‡∏£‡∏≤‡∏ô‡πÅ‡∏ã‡∏Å‡∏ä‡∏±‡∏ô‡∏™‡∏±‡πâ‡∏ô‡πÜ) ===
def update_status_and_slip_info(order_id, new_status, slip_image_path, slip_transaction_id):
    try:
        order = Order.query.filter_by(id=order_id).first()
        if not order:
            print(f"‚ùå [ERROR] Order ID {order_id} not found for status update.")
            return False

        # ‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ã‡πâ‡∏≥‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô (‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏î‡∏¥‡∏°)
        changed = False
        if order.status != new_status:
            order.status = new_status
            changed = True
        if order.slip_image != slip_image_path:
            order.slip_image = slip_image_path
            changed = True

        if not changed:
            print(f"‚ÑπÔ∏è Order ID: {order_id} no changes applied.")
            return True

        order.updated_at = datetime.now()
        db.session.commit()
        print(f"‚úÖ Order ID: {order_id} status updated to '{new_status}' with slip info.")
        return True
    except Exception as e:
        db.session.rollback()
        print(f"‚ùå Error updating order status for ID {order_id}: {e}")
        return False

# === Ad status updater: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡πÅ‡∏ö‡∏ö‡∏à‡∏á‡πÉ‡∏à ‡πÑ‡∏°‡πà‡∏¢‡∏∏‡πà‡∏á‡∏ß‡∏±‡∏ô‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏/‡∏ß‡∏±‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏° ===
def update_ad_status(ad_id, new_status):
    try:
        ad = Ad.query.filter_by(id=ad_id).first()
        if not ad:
            print(f"‚ùå [ERROR] Ad ID {ad_id} not found for status update.")
            return False

        if ad.status == new_status:
            print(f"‚ÑπÔ∏è Ad ID: {ad_id} already in status '{new_status}'.")
            return True

        ad.status = new_status
        ad.updated_at = datetime.now()
        db.session.commit()
        print(f"‚úÖ Ad ID: {ad_id} status updated to '{new_status}'.")
        return True
    except Exception as e:
        db.session.rollback()
        print(f"‚ùå Error updating ad status for ID {ad_id}: {e}")
        return False

# === Ad renew updater: ‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏¢‡∏∏ + ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ (‡∏Ñ‡∏∏‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏) ===
def update_ad_for_renewal(ad_id, new_status, new_expiration_date):
    try:
        ad = Ad.query.filter_by(id=ad_id).first()
        if not ad:
            print(f"‚ùå [ERROR] Ad ID {ad_id} not found for renewal update.")
            return False

        changed = False
        if ad.status != new_status:
            ad.status = new_status
            changed = True
        if ad.expiration_date != new_expiration_date:
            ad.expiration_date = new_expiration_date
            changed = True

        if not changed:
            print(f"‚ÑπÔ∏è Ad ID: {ad_id} no changes applied (status/expiration unchanged).")
            return True

        ad.updated_at = datetime.now()
        db.session.commit()
        print(f"‚úÖ Ad ID: {ad_id} status updated to '{new_status}' and expiration date extended to {new_expiration_date.strftime('%Y-%m-%d')}.")
        return True
    except Exception as e:
        db.session.rollback()
        print(f"‚ùå Error updating ad for renewal ID {ad_id}: {e}")
        return False

# === Store PromptPay payload on Order: ‡πÄ‡∏Å‡πá‡∏ö QR payload ‡πÑ‡∏ß‡πâ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á ===
def update_order_with_promptpay_payload_db(order_id, payload_to_store_in_db):
    try:
        order = Order.query.filter_by(id=order_id).first()
        if not order:
            print(f"‚ùå [ERROR] Order ID {order_id} not found for payload update.")
            return False

        if order.promptpay_qr_payload == payload_to_store_in_db:
            print(f"‚ÑπÔ∏è Order ID: {order_id} payload unchanged.")
            return True

        order.promptpay_qr_payload = payload_to_store_in_db
        order.updated_at = datetime.now()
        db.session.commit()
        print(f"‚úÖ Order ID: {order_id} updated with PromptPay payload.")
        return True
    except Exception as e:
        db.session.rollback()
        print(f"‚ùå Error updating order with PromptPay payload: {e}")
        return False

# === Create Ad by paid Order: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏ú‡∏π‡∏Å‡∏Å‡∏±‡∏ö‡∏≠‡∏≠‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ä‡∏≥‡∏£‡∏∞‡πÅ‡∏•‡πâ‡∏ß ===
def create_advertisement_db(order_data):
    try:
        now = datetime.now()
        default_title = f"Advertisement for Order {order_data['id']}"
        default_content = "This is a new advertisement pending admin approval after payment."
        ad_show_at = order_data.get('show_at', now)

        ad = Ad(
            user_id=order_data['user_id'],
            order_id=order_data['id'],
            title=default_title,
            content=default_content,
            link="",
            image="",
            status='paid',
            created_at=now,
            updated_at=now,
            show_at=ad_show_at
        )
        db.session.add(ad)
        db.session.commit()
        print(f"üöÄ Advertisement ID: {ad.id} created for Order ID: {order_data['id']} with status 'paid'.")
        return ad.id
    except Exception as e:
        db.session.rollback()
        print(f"‚ùå Error creating advertisement for Order ID {order_data.get('id')}: {e}")
        return None

# ==================== SLIP & PROMPTPAY FUNCTIONS (from Slip.py) ====================


# === Generate PromptPay QR for a given order (‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç/‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å payload/‡∏Ñ‡∏∑‡∏ô QR base64) ===
def generate_promptpay_qr_for_order(order_id):
    order = find_order_by_id(order_id)
    if not order:
        print(f"‚ùå [WARN] Order ID {order_id} not found for QR generation.")
        return {"success": False, "message": "Order not found."}

    is_new_ad_approved = order["status"] == 'approved' and order.get("renew_ads_id") is None
    is_renewal_ad_pending = order["status"] == 'pending' and order.get("renew_ads_id") is not None

    if is_new_ad_approved or is_renewal_ad_pending:
        print(f"‚úÖ [INFO] Order ID {order_id} is eligible for QR generation. Status: '{order['status']}', Renew Ad: {order.get('renew_ads_id')}.")
    else:
        log_message = f"‚ùå [WARN] Cannot generate QR for order {order_id}. Current status: '{order['status']}'."
        if order["status"] == 'pending' and order.get("renew_ads_id") is None:
            log_message += " (New ad order not yet approved by admin)."
            print(log_message)
            return {"success": False, "message": "Cannot generate a QR code. Please wait until an admin approves the content."}
        else:
            log_message += " (Invalid status for QR generation)."
            print(log_message)
            return {"success": False, "message": "Cannot generate a QR code. Invalid order status."}

    # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô amount ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô/‡∏ï‡∏¥‡∏î‡∏•‡∏ö/‡πÄ‡∏õ‡πá‡∏ô None
    try:
        amount = float(order["amount"])
        if amount <= 0:
            print(f"‚ùå [ERROR] Invalid amount for order {order_id}: {order['amount']}")
            return {"success": False, "message": "Invalid order amount."}
    except Exception as e:
        print(f"‚ùå [ERROR] Amount parse failed for order {order_id}: {e}")
        return {"success": False, "message": "Invalid order amount."}

    # ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    if promptpay_qrcode is None:
        print(f"‚ùå [ERROR] promptpay_qrcode library not found.")
        return {"success": False, "message": "PromptPay library not found. Please install it first."}

    # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ PROMPTPAY_ID ‡πÉ‡∏ô env
    promptpay_id = os.getenv("PROMPTPAY_ID")
    if not promptpay_id:
        print(f"‚ùå [ERROR] PROMPTPAY_ID environment variable not set.")
        return {"success": False, "message": "PromptPay ID not found in settings."}

    try:
        original_scannable_payload = promptpay_qrcode.generate_payload(promptpay_id, amount)
    except Exception as e:
        print(f"‚ùå [ERROR] Payload generation failed for order {order_id}: {e}")
        return {"success": False, "message": "Failed to generate QR payload."}

    if not update_order_with_promptpay_payload_db(order_id, original_scannable_payload):
        print(f"‚ùå [ERROR] Failed to save QR Code payload to database for order {order_id}.")
        return {"success": False, "message": "Failed to save QR code data to the database."}

    try:
        print(f"‚úÖ Generated PromptPay payload (stored in DB): {original_scannable_payload}")
        qr = qrcode.QRCode(
            version=1,
            error_correction=ERROR_CORRECT_H,
            box_size=10,
            border=4,
        )
        qr.add_data(original_scannable_payload)
        qr.make(fit=True)
        img = qr.make_image(fill_color="black", back_color="white")
        buffered = io.BytesIO()
        # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á qrcode ‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà/‡πÄ‡∏Å‡πà‡∏≤
        (img.get_image() if hasattr(img, 'get_image') else img).save(buffered, "PNG")
        img_b64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
        return {"success": True, "message": "QR code generated successfully.", "qrcode_base64": img_b64, "payload": original_scannable_payload}
    except Exception as e:
        print(f"‚ùå [ERROR] QR image encode failed for order {order_id}: {e}")
        return {"success": False, "message": "Failed to render QR image."}

# === Slip upload permission check (‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡∏™‡∏•‡∏¥‡∏õ‡πÑ‡∏î‡πâ) ===
def can_upload_slip(order):
    if not order:
        return False
    is_new_ad_approved = order["status"] == 'approved' and order.get("renew_ads_id") is None
    is_renewal_ad_pending = order["status"] == 'pending' and order.get("renew_ads_id") is not None
    return is_new_ad_approved or is_renewal_ad_pending

# === Thai date formatter (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö str, date, datetime) ===
def format_thai_date(date_obj):
    if not isinstance(date_obj, (datetime, date, str)):
        return "Incorrect Format"

    if isinstance(date_obj, str):
        try:
            date_obj = datetime.fromisoformat(date_obj)
        except ValueError:
            try:
                date_obj = datetime.strptime(date_obj, '%Y-%m-%d %H:%M:%S')
            except ValueError:
                try:
                    date_obj = datetime.strptime(date_obj, '%Y-%m-%d').date()
                except ValueError:
                    return "Incorrect Format"

    if isinstance(date_obj, date) and not isinstance(date_obj, datetime):
        date_obj = datetime(date_obj.year, date_obj.month, date_obj.day)

    thai_year = date_obj.year + 543
    try:
        formatted_date = date_obj.strftime(f'%d %B {thai_year}')
    except ValueError:
        thai_month_names = [
            "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°", "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå", "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°", "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô", "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°", "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô",
            "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°", "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°", "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô", "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°", "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô", "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°"
        ]
        formatted_date = f"{date_obj.day} {thai_month_names[date_obj.month - 1]} {thai_year}"
    return formatted_date

# === Notification writer: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡πÄ‡∏õ‡πá‡∏ô notification ===
def notify_ads_status_change(db, ad_id: int, new_status: str, admin_notes: str = None, duration_days_from_renewal: int = None) -> bool:
    try:
        ads_query = text('SELECT user_id, expiration_date FROM ads WHERE id = :ad_id')
        ads_result = db.session.execute(ads_query, {'ad_id': ad_id}).fetchone()
        if not ads_result:
            print(f"‚ùå [WARN] notify_ads_status_change: Ad ID {ad_id} not found in 'ads' table.")
            return False

        user_id = ads_result[0]
        expiration_date = ads_result[1]

        content = ''
        duration_from_package_db = None

        # ‡∏´‡∏≤‡∏ß‡∏±‡∏ô duration ‡∏à‡∏≤‡∏Å order ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡πà‡∏≤‡∏¢/‡∏£‡∏≠‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡πà‡∏≤‡∏¢
        if duration_days_from_renewal is None:
            order_package_query = text("""
                SELECT ap.duration_days, o.status as order_status
                FROM orders o
                JOIN ad_packages ap ON o.package_id = ap.package_id
                WHERE o.renew_ads_id = :ad_id
                  AND o.status IN ('paid', 'approved_payment_waiting')
                ORDER BY o.created_at DESC
                LIMIT 1
            """)
            package_info_result = db.session.execute(order_package_query, {'ad_id': ad_id}).fetchone()
            if package_info_result:
                duration_from_package_db = package_info_result[0]
                order_status_found = package_info_result[1]
                print(f"‚úÖ [INFO] Found package duration {duration_from_package_db} days from order for Ad ID {ad_id} with order status '{order_status_found}'.")
            else:
                print(f"‚ö†Ô∏è [WARN] No 'paid' or 'approved_payment_waiting' order found linked to Ad ID {ad_id} via renew_ads_id to determine package duration.")

        if new_status == 'active':
            duration_to_use = duration_days_from_renewal if duration_days_from_renewal is not None else duration_from_package_db
            if duration_to_use is not None:
                formatted_expiration_date = format_thai_date(expiration_date)
                content = f"Your ad has been successfully renewed for {duration_to_use} days. This ad's expiration date is extended to {formatted_expiration_date}"
            else:
                content = 'Your ad has been approved for display'
        elif new_status == 'paid':
            content = 'Your ad payment has been completed. Waiting for admin review'
        elif new_status == 'approved':
            content = 'Your ad has been reviewed. Please transfer payment to display it'
        elif new_status == 'rejected':
            content = f'Your ad was rejected. Reason: {admin_notes or "-"}'
        elif new_status == 'expired':
            content = 'Your ad has expired'
        elif new_status == 'expiring_soon':
            content = 'Your ad will expire in 3 days. Please renew to ensure continuous display'
        else:
            content = f'Your ad status has changed to {new_status}'

        insert_notification_query = text("""
            INSERT INTO notifications (user_id, action_type, content, ads_id)
            VALUES (:user_id, 'ads_status_change', :content, :ads_id)
        """)
        db.session.execute(insert_notification_query, {
            'user_id': user_id,
            'content': content,
            'ads_id': ad_id
        })
        db.session.commit()
        print(f"‚úÖ [INFO] Notification saved successfully for Ad ID {ad_id}. Content: '{content}'")
        return True

    except Exception as e:
        print(f"‚ùå [ERROR] An unexpected error occurred in notify_ads_status_change for Ad ID {ad_id}: {e}")
        db.session.rollback()
        return False

# === Slip verify + update order/ad: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å SlipOK, ‡∏ï‡∏£‡∏ß‡∏à‡∏ú‡∏•, ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞, ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô ===
def verify_payment_and_update_status(order_id, slip_image_path, payload_from_client, db):
    """
    Call SlipOK and update our order/ad. Return concise EN messages for client.
    """
    print(f"[INFO] process order={order_id}")

    order = find_order_by_id(order_id)
    if not order:
        return {"success": False, "message": "Order not found."}
    if not can_upload_slip(order):
        return {"success": False, "message": "Slip upload not allowed for this order status."}
    if not os.path.exists(slip_image_path):
        return {"success": False, "message": "Slip file missing on server."}

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏¢‡∏∏‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
    ad_related = None
    if order.get("renew_ads_id") is not None:
        ad_related = find_ad_by_id(order["renew_ads_id"])
        if not ad_related:
            return {"success": False, "message": "Ad for renewal not found."}
    else:
        already = find_ad_by_order_id(order_id)
        if already and already.get('status') in ['active']:
            return {"success": False, "message": "This order was already processed."}

    # SlipOK config
    SLIP_OK_API_ENDPOINT = os.getenv("SLIP_OK_API_ENDPOINT", "https://api.slipok.com/api/line/apikey/49130")
    SLIP_OK_API_KEY = os.getenv("SLIP_OK_API_KEY", "SLIPOKKBE52WN")  # ‡∏ï‡∏±‡πâ‡∏á env ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ä‡∏±‡∏ô

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å SlipOK
    try:
        with open(slip_image_path, 'rb') as img_file:
            files = {'files': (os.path.basename(slip_image_path), img_file, 'image/jpeg')}
            data = {'log': 'true', 'amount': str(float(order["amount"]))}
            headers = {'x-authorization': SLIP_OK_API_KEY}
            r = requests.post(SLIP_OK_API_ENDPOINT, files=files, data=data, headers=headers, timeout=30)
            text_body = r.text
            r.raise_for_status()
            resp = r.json()
    except requests.exceptions.Timeout:
        print(f"[ERR] SlipOK timeout order={order_id}")
        return {"success": False, "message": "Verification service timeout. Please try again."}
    except requests.exceptions.HTTPError as e:
        msg = "Verification failed."
        try:
            j = r.json()
            code = j.get('code')
            slipok_msg = j.get('message', '')
            if code == 1002:
                msg = "Verification provider rejected our credentials. Contact support."
            elif code == 1012:
                msg = "Duplicate slip. This slip was already submitted."
            else:
                msg = slipok_msg or msg
        except Exception:
            msg = f"Verification failed: HTTP {r.status_code}"
        print(f"[ERR] SlipOK HTTP {r.status_code} order={order_id} msg={msg} body={text_body[:200]}")
        return {"success": False, "message": msg}
    except requests.exceptions.RequestException as e:
        print(f"[ERR] SlipOK connect error order={order_id}: {e}")
        return {"success": False, "message": "Cannot reach verification service. Please try again."}
    except Exception as e:
        print(f"[ERR] SlipOK call unexpected order={order_id}: {e}")
        return {"success": False, "message": "Unexpected error during verification."}

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏ú‡∏• SlipOK
    if not resp.get("success"):
        msg = resp.get("message") or "Verification failed."
        code = resp.get("code")
        if code == 1012:
            msg = "Duplicate slip. This slip was already submitted."
        return {"success": False, "message": msg}

    data = resp.get("data") or {}
    slip_amount = float(data.get("amount", 0))
    trans_ref = data.get("transRef")
    if not trans_ref:
        return {"success": False, "message": "Slip verification returned no transaction ID."}

    if abs(slip_amount - float(order.get("amount"))) > 0.01:
        return {"success": False, "message": f"Incorrect amount. Expected {order.get('amount'):.2f}, got {slip_amount:.2f}."}

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ (order/ad/notification)
    try:
        ok = update_status_and_slip_info(order_id, "paid", slip_image_path, trans_ref)
        if not ok:
            raise RuntimeError("Update order failed")

        ad_id_to_return = None
        if order.get("renew_ads_id") is not None:
            # ‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏¢‡∏∏: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡∏±‡∏ô‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡πÉ‡∏´‡∏°‡πà
            duration_days = get_ad_package_duration(order["package_id"])
            if duration_days is None:
                raise RuntimeError("Missing package duration")
            current_ad = find_ad_by_id(order["renew_ads_id"])
            start = current_ad.get('expiration_date') or datetime.now().date()
            if isinstance(start, datetime):
                start = start.date()
            new_exp = datetime.combine(start, datetime.min.time()).date() + timedelta(days=duration_days)
            if not update_ad_for_renewal(current_ad['id'], "active", new_exp):
                raise RuntimeError("Renewal update failed")
            ad_id_to_return = current_ad['id']
            notify_ads_status_change(db, ad_id_to_return, 'active')
            message = f"Payment verified. Your ad was renewed for {duration_days} days."
        else:
            ad = find_ad_by_order_id(order_id)
            if ad:
                if not update_ad_status(ad['id'], "paid"):
                    raise RuntimeError("Ad status update failed")
                ad_id_to_return = ad['id']
            else:
                new_id = create_advertisement_db(order)
                if not new_id:
                    raise RuntimeError("Ad creation failed")
                ad_id_to_return = new_id
            notify_ads_status_change(db, ad_id_to_return, 'paid')
            message = "Payment verified. Please wait for admin to activate the ad."

        db.session.commit()
        print(f"[INFO] order={order_id} verified, ad_id={ad_id_to_return}, transRef={trans_ref}")
        return {"success": True, "message": message, "ad_id": ad_id_to_return}

    except Exception as e:
        try:
            if db and hasattr(db, 'session'):
                db.session.rollback()
        except Exception as rb:
            print(f"[WARN] rollback error: {rb}")
        print(f"[ERR] post-verify update failed order={order_id}: {e}")
        return {"success": False, "message": "Internal update failed after verification."}

# === Expiring-soon notifier: ‡πÉ‡∏™‡πà‡∏•‡πá‡∏≠‡∏Å‡∏Å‡∏±‡∏ô‡∏£‡∏±‡∏ô‡∏ã‡πâ‡∏≠‡∏ô/‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡∏ã‡πâ‡∏≥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ===
def check_ads_expiring_soon(db, today_date: date = None):
    if today_date is None:
        today_date = date.today()
    target_date = today_date + timedelta(days=3)

    lockname = f"expiring_run_{today_date.isoformat()}"
    conn = db.engine.raw_connection()
    try:
        cur = conn.cursor()
        # ‡∏•‡πá‡∏≠‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏≠‡∏ö ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ã‡πâ‡∏≠‡∏ô
        cur.execute("SELECT GET_LOCK(%s, 5)", (lockname,))
        got = cur.fetchone()[0]
        if not got:
            print(f"[INFO] skip: another run holds lock {lockname}")
            return 0, []

        # ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ anti-join ‡∏ó‡∏µ‡πà‡∏≠‡∏¥‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á DB ‡πÄ‡∏≠‡∏á (CURDATE())
        sql = """
        INSERT INTO notifications (user_id, action_type, content, ads_id, created_at, read_status)
        SELECT a.user_id,
               'ads_status_change',
               'Your ad will expire in 3 days. Please renew to ensure continuous display',
               a.id,
               NOW(),
               0
          FROM ads a
     LEFT JOIN notifications n
            ON n.ads_id = a.id
           AND n.action_type = 'ads_status_change'
           AND n.content = 'Your ad will expire in 3 days. Please renew to ensure continuous display'
           AND DATE(n.created_at) = CURDATE()
         WHERE DATE(a.expiration_date) = %s
           AND a.status IN ('active')
           AND n.id IS NULL
        """
        cur.execute(sql, (target_date.isoformat(),))
        inserted = cur.rowcount or 0
        conn.commit()

        if inserted == 0:
            print("[INFO] no eligible ads (already notified today or none match)")
        else:
            print(f"[INFO] notified {inserted} ad(s) for expiring_soon")

        return inserted, []
    except Exception as e:
        conn.rollback()
        print("[ERR] expiring_soon atomic insert:", e)
        return 0, []
    finally:
        try:
            cur.execute("SELECT RELEASE_LOCK(%s)", (lockname,))
            conn.commit()
        except Exception:
            conn.rollback()
        try: cur.close()
        except: pass
        try: conn.close()
        except: pass

# === Scheduler: ‡∏£‡∏±‡∏ô‡πÄ‡∏ä‡πá‡∏Å‡∏´‡∏°‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡∏ó‡∏∏‡∏Å‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏á‡∏Ñ‡∏∑‡∏ô‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏û‡∏£‡πâ‡∏≠‡∏° app context ===
def start_expiry_scheduler(app, db):
    tz = pytz.timezone('Asia/Bangkok')
    sched = BackgroundScheduler(timezone=tz)

    def job():
        with app.app_context():
            inserted, _ = check_ads_expiring_soon(db)
            if inserted == 0:
                print("[CRON] no eligible ads (already notified today or none match)")
            else:
                print(f"[CRON] notified {inserted} ad(s) for expiring_soon")

    sched.add_job(job, 'cron', hour=0, minute=0)
    sched.start()
    print("‚è∞ Expiry scheduler started (runs 00:00 Asia/Bangkok)")



# ==================== FLASK ROUTES ====================
ALLOWED_IMAGE_EXTS = {'.jpg', '.jpeg', '.png', '.webp'}
ALLOWED_VIDEO_EXTS = {'.mp4', '.mov', '.avi', '.mkv'}
MAX_UPLOAD_BYTES = 20 * 1024 * 1024  # 20MB ‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö API)

def _ext_ok(name, allowed):
    return os.path.splitext(name)[1].lower() in allowed

def _unique_name(filename: str) -> str:
    name = secure_filename(filename)
    base, ext = os.path.splitext(name)
    token = secrets.token_hex(4)
    return f"{int(time.time())}_{token}{ext.lower()}"

def _allowed_ext(filename: str, allowed: set) -> bool:
    return os.path.splitext(filename)[1].lower() in allowed

def _save_upload(file, allowed_exts: set, folder: str) -> str:
    if not file or not file.filename:
        raise ValueError("empty_file")
    if hasattr(file, 'content_length') and file.content_length and file.content_length > MAX_UPLOAD_BYTES:
        raise ValueError("file_too_large")
    if not _allowed_ext(file.filename, allowed_exts):
        raise ValueError("unsupported_extension")
    fname = _unique_name(file.filename)
    path = os.path.join(folder, fname)
    file.save(path)
    return fname, path

# ==================== Recommendation Route ====================
@app.route('/ai/recommend', methods=['POST'])
@verify_token
def recommend():
    try:
        # === ‡∏≠‡πà‡∏≤‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡πá‡∏Å refresh ===
        user_id = request.user_id
        now = datetime.now()
        refresh_requested = request.args.get('refresh', 'false').lower() == 'true'

        # === ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ cache invalidation ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏™‡∏≠‡∏∑‡πà‡∏ô‡∏û‡∏±‡∏á ===
        if refresh_requested and user_id in recommendation_cache:
            try:
                del recommendation_cache[user_id]
            except Exception:
                recommendation_cache.pop(user_id, None)
            print(f"Cache for user_id: {user_id} invalidated due to client-side refresh request. Impression history RETAINED.")

        # === ‡πÉ‡∏ä‡πâ cache ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏™‡∏î‡∏°‡∏≤‡∏Å ‡πÜ ‡∏•‡∏î‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ===
        if user_id in recommendation_cache and not refresh_requested:
            cached_data, cache_timestamp = recommendation_cache[user_id]
            if (now - cache_timestamp).total_seconds() < (CACHE_EXPIRY_TIME_SECONDS / 2):
                print(f"Returning VERY FRESH cached recommendations for user_id: {user_id}")
                return jsonify(cached_data)
            print(f"Cached recommendations for user_id: {user_id} are still valid but slightly old. Recalculating for freshness.")

        # === ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å DB ===
        content_based_data, collaborative_data = load_data_from_db()

        # === ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏≤‡∏¢/‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô) ===
        try:
            knn = joblib.load('KNN_Model.pkl')
            collaborative_model = joblib.load('Collaborative_Model.pkl')
            tfidf = joblib.load('TFIDF_Model.pkl')

            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏¥‡∏°‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°
            if 'NormalizedEngagement' not in content_based_data.columns:
                content_based_data = normalize_engagement(content_based_data, user_column='owner_id', engagement_column='PostEngagement')

            if 'CommentCount' not in content_based_data.columns:
                content_based_data['CommentCount'] = analyze_comments(content_based_data['Comments'])
                m = content_based_data['CommentCount'].max()
                content_based_data['NormalizedCommentCount'] = (content_based_data['CommentCount'] / m) if m and m > 0 else 0.0

            if 'WeightedEngagement' not in content_based_data.columns:
                content_based_data['WeightedEngagement'] = content_based_data['NormalizedEngagement'] + content_based_data['NormalizedCommentCount']

        except FileNotFoundError as e:
            print(f"Error loading models: {e}")
            return jsonify({"error": "Model files not found"}), 500
        except Exception as e:
            print(f"Error initializing models: {e}")
            return jsonify({"error": "Model initialization failed"}), 500

        # === ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡πÄ‡∏î‡∏¥‡∏°) ===
        categories = ['Electronics_Gadgets', 'Furniture', 'Outdoor_Gear', 'Beauty_Products', 'Accessories']

        # === ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏ö‡∏ö Hybrid ===
        recommendations = recommend_hybrid(
            user_id,
            content_based_data,
            collaborative_model,
            knn,
            tfidf,
            categories,
            alpha=0.8,
            beta=0.2
        )
        if not recommendations:
            return jsonify({"error": "No recommendations found"}), 404

        # === ‡∏î‡∏∂‡∏á interaction ‡∏Ç‡∏≠‡∏á user ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏á ===
        user_interactions = collaborative_data[collaborative_data['user_id'] == user_id]['post_id'].tolist()

        # === ‡πÉ‡∏ä‡πâ impression history ‡∏ó‡∏µ‡πà‡πÅ‡∏Ñ‡∏ä‡πÑ‡∏ß‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ß‡∏ô‡∏ã‡πâ‡∏≥‡πÇ‡∏û‡∏™‡∏ï‡πå‡πÄ‡∏î‡∏¥‡∏° ===
        current_impression_history = impression_history_cache.get(user_id, [])
        print(f"Current Impression History for user {user_id}: {[entry['post_id'] for entry in current_impression_history]}")

        # === ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ===
        final_recommendations_ids = split_and_rank_recommendations(
            recommendations,
            user_interactions,
            current_impression_history,
            len(content_based_data)
        )

        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ id ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö 404 ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if not final_recommendations_ids:
            return jsonify({"error": "No recommendations after ranking"}), 404

        # ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î IN ‡πÉ‡∏´‡πâ‡∏û‡∏≠‡πÄ‡∏´‡∏°‡∏≤‡∏∞ ‡∏Å‡∏±‡∏ô param ‡∏£‡∏∞‡πÄ‡∏ö‡∏¥‡∏î (‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏û‡∏≠ feed ‡∏´‡∏ô‡πâ‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå‡πÅ‡∏£‡∏Å)
        MAX_IDS = 200
        final_ids_slice = final_recommendations_ids[:MAX_IDS]

        # === ‡∏î‡∏∂‡∏á‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö AI (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á SQL ‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ß‡πà‡∏≤‡∏á) ===
        placeholders = ', '.join([f':id_{i}' for i in range(len(final_ids_slice))])
        query = text(f"""
            SELECT posts.*, users.username, users.picture,
                   (SELECT COUNT(*) FROM likes WHERE post_id = posts.id AND user_id = :user_id) AS is_liked
            FROM posts 
            JOIN users ON posts.user_id = users.id
            WHERE posts.status = 'active' AND posts.id IN ({placeholders})
        """) if final_ids_slice else text("""
            SELECT posts.*, users.username, users.picture,
                   (SELECT 0) AS is_liked
            FROM posts 
            JOIN users ON posts.user_id = users.id
            WHERE 1=0
        """)

        if final_ids_slice:
            params = {'user_id': user_id, **{f'id_{i}': pid for i, pid in enumerate(final_ids_slice)}}
            result = db.session.execute(query, params).fetchall()
        else:
            result = []

        # === ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà AI ‡πÉ‡∏´‡πâ‡∏°‡∏≤ ===
        id_to_rank = {pid: idx for idx, pid in enumerate(final_recommendations_ids)}
        posts = [row._mapping for row in result]
        sorted_posts = sorted(posts, key=lambda x: id_to_rank.get(x['id'], 10**9))

        # === ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô payload ‡∏ù‡∏±‡πà‡∏á client ===
        output = []
        for post in sorted_posts:
            try:
                updated = post['updated_at']
                # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ driver ‡∏Ñ‡∏∑‡∏ô naive datetime
                if hasattr(updated, 'tzinfo') and updated.tzinfo is None:
                    updated = updated.replace(tzinfo=timezone.utc)
                iso_updated = updated.astimezone(timezone.utc).replace(microsecond=0).isoformat() + 'Z'
            except Exception:
                iso_updated = datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

            output.append({
                "id": post['id'],
                "userId": post['user_id'],
                "title": post['Title'],
                "content": post['content'],
                "updated": iso_updated,
                "photo_url": json.loads(post.get('photo_url', '[]') or '[]'),
                "video_url": json.loads(post.get('video_url', '[]') or '[]'),
                "userName": post['username'],
                "userProfileUrl": post['picture'],
                "is_liked": (post['is_liked'] or 0) > 0
            })

        # === ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô cache ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ===
        recommendation_cache[user_id] = (output, now)

        # === ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï impression history ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏ï‡πÄ‡∏Å‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô ===
        if user_id not in impression_history_cache:
            impression_history_cache[user_id] = []

        existing_set = {entry['post_id'] for entry in impression_history_cache[user_id]}
        new_impressions = [{'post_id': pid, 'timestamp': now} for pid in final_recommendations_ids if pid not in existing_set]
        impression_history_cache[user_id].extend(new_impressions)
        impression_history_cache[user_id] = sorted(
            impression_history_cache[user_id],
            key=lambda x: x['timestamp'],
            reverse=True
        )[:IMPRESSION_HISTORY_MAX_ENTRIES]

        print(f"Updated Impression History for user {user_id}: {[entry['post_id'] for entry in impression_history_cache[user_id]]}")
        return jsonify(output)

    except KeyError as e:
        print(f"KeyError in recommend function: {e}")
        return jsonify({"error": f"KeyError: {e}"}), 500
    except Exception as e:
        print("Error in recommend function:", e)
        return jsonify({"error": "Internal Server Error"}), 500


# === NSFW Detection Route: create post ===
@app.route('/ai/posts/create', methods=['POST'])
def create_post():
    try:
        user_id = request.form.get('user_id')
        content = request.form.get('content')
        category = request.form.get('category')
        title = request.form.get('Title')
        product_name = request.form.get('ProductName')
        photos = request.files.getlist('photo')
        videos = request.files.getlist('video')

        if not user_id:
            return jsonify({"error": "You are not authorized to create a post for this user"}), 403

        photo_urls, invalid_photos = [], []
        print(f"Processing {len(photos)} photos...")

        for photo in photos:
            if not photo or not photo.filename:
                continue
            photo_path = None
            try:
                fname, photo_path = _save_upload(photo, ALLOWED_IMAGE_EXTS, UPLOAD_FOLDER)
                print(f"Processing photo: {fname}")
                is_nude, result = nude_predict_image(photo_path)

                if is_nude:
                    print(f"NSFW detected in {fname}")
                    if os.path.exists(photo_path):
                        os.remove(photo_path)
                    invalid_photos.append({
                        "filename": fname,
                        "reason": "‡∏û‡∏ö‡∏†‡∏≤‡∏û‡πÇ‡∏õ‡πä (Hentai ‡∏´‡∏£‡∏∑‡∏≠ Pornography > 20%)",
                        "details": result
                    })
                else:
                    print(f"Photo {fname} is safe")
                    photo_urls.append(f'/uploads/{fname}')
            except Exception as e:
                print(f"Error processing photo {getattr(photo, 'filename', '?')}: {e}")
                if photo_path and os.path.exists(photo_path):
                    os.remove(photo_path)
                invalid_photos.append({
                    "filename": getattr(photo, 'filename', '?'),
                    "reason": "Unable to process the image.",
                    "details": {"error": str(e)}
                })

        print(f"Invalid photos found: {len(invalid_photos)}")
        print(f"Valid photos: {len(photo_urls)}")

        if invalid_photos:
            print("‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô user: ‡∏û‡∏ö‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°")
            return jsonify({
                "status": "warning",
                "message": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
                "invalid_photos": invalid_photos,
                "valid_photos": photo_urls
            }), 400

        video_urls = []
        for video in videos:
            if not video or not video.filename:
                continue
            try:
                vname, vpath = _save_upload(video, ALLOWED_VIDEO_EXTS, UPLOAD_FOLDER)
                video_urls.append(f'/uploads/{vname}')
            except Exception as e:
                print(f"Skip invalid video {getattr(video, 'filename', '?')}: {e}")
                # ‡πÑ‡∏°‡πà fail ‡∏ó‡∏±‡πâ‡∏á‡πÇ‡∏û‡∏™‡∏ï‡πå‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏û‡∏±‡∏á

        photo_urls_json = json.dumps(photo_urls)
        video_urls_json = json.dumps(video_urls)

        try:
            insert_query = text("""
                INSERT INTO posts (user_id, Title, content, ProductName, CategoryID, photo_url, video_url, status, updated_at)
                VALUES (:user_id, :title, :content, :product_name, :category_id, :photo_urls, :video_urls, 'active', NOW())
            """)
            result = db.session.execute(insert_query, {
                'user_id': user_id,
                'title': title,
                'content': content,
                'product_name': product_name,
                'category_id': category,
                'photo_urls': photo_urls_json,
                'video_urls': video_urls_json
            })
            db.session.commit()

            post_id = getattr(result, "lastrowid", None)
            if not post_id:
                # fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≤‡∏á dialect
                post_id = db.session.execute(text("SELECT LAST_INSERT_ID()")).scalar()

            print(f"Post created successfully with ID: {post_id}, {len(photo_urls)} photos and {len(video_urls)} videos")

            return jsonify({
                "post_id": post_id,
                "user_id": user_id,
                "content": content,
                "category": category,
                "Title": title,
                "ProductName": product_name,
                "video_urls": video_urls,
                "photo_urls": photo_urls
            }), 201

        except Exception as db_error:
            print(f"Database error: {db_error}")
            db.session.rollback()
            # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏ã‡∏ü‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ç‡∏¢‡∏∞‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ DB fail
            for url in photo_urls:
                try:
                    path = os.path.join(UPLOAD_FOLDER, os.path.basename(url))
                    if os.path.exists(path):
                        os.remove(path)
                except Exception:
                    pass
            for url in video_urls:
                try:
                    path = os.path.join(UPLOAD_FOLDER, os.path.basename(url))
                    if os.path.exists(path):
                        os.remove(path)
                except Exception:
                    pass
            return jsonify({"error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ"}), 500

    except Exception as error:
        print("Internal server error:", str(error))
        return jsonify({"error": "Internal server error"}), 500


# === Update post: ‡∏ú‡∏™‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏° + ‡πÉ‡∏´‡∏°‡πà ‡∏ï‡∏£‡∏ß‡∏à NSFW ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏°‡πà ===
@app.route('/ai/posts/<int:id>', methods=['PUT'])
def update_post(id):
    try:
        Title = request.form.get('Title')
        content = request.form.get('content')
        ProductName = request.form.get('ProductName')
        CategoryID = request.form.get('CategoryID')
        user_id = request.form.get('user_id')

        existing_photos_str = request.form.get('existing_photos')
        existing_videos_str = request.form.get('existing_videos')

        try:
            existing_photos = json.loads(existing_photos_str) if existing_photos_str else []
        except Exception:
            existing_photos = []
        try:
            existing_videos = json.loads(existing_videos_str) if existing_videos_str else []
        except Exception:
            existing_videos = []

        photos = request.files.getlist('photo')
        videos = request.files.getlist('video')

        if not user_id:
            return jsonify({"error": "You are not authorized to update this post"}), 403

        photo_urls = list(existing_photos) if existing_photos else []
        invalid_photos = []

        for photo in photos:
            photo_path = None
            if not photo or not photo.filename:
                continue
            try:
                fname, photo_path = _save_upload(photo, ALLOWED_IMAGE_EXTS, UPLOAD_FOLDER)
                print(f"Processing new photo for update: {fname}")
                is_nude, result = nude_predict_image(photo_path)
                if is_nude:
                    print(f"NSFW detected in {fname} during update")
                    if os.path.exists(photo_path):
                        os.remove(photo_path)
                    invalid_photos.append({
                        "filename": fname,
                        "reason": "‡∏û‡∏ö‡∏†‡∏≤‡∏û‡πÇ‡∏õ‡πä (Hentai ‡∏´‡∏£‡∏∑‡∏≠ Pornography > 20%)",
                        "details": result
                    })
                else:
                    print(f"New photo {fname} is safe")
                    photo_urls.append(f'/uploads/{fname}')
            except Exception as e:
                print(f"Error processing photo {getattr(photo, 'filename', '?')} during update: {e}")
                if photo_path and os.path.exists(photo_path):
                    os.remove(photo_path)
                invalid_photos.append({
                    "filename": getattr(photo, 'filename', '?'),
                    "reason": "Unable to process the image.",
                    "details": {"error": str(e)}
                })

        if invalid_photos:
            print("‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô user: ‡∏û‡∏ö‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï")
            return jsonify({
                "status": "warning",
                "message": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
                "invalid_photos": invalid_photos,
                "valid_photos": photo_urls
            }), 400

        video_urls = list(existing_videos) if existing_videos else []
        for video in videos:
            if not video or not video.filename:
                continue
            try:
                vname, vpath = _save_upload(video, ALLOWED_VIDEO_EXTS, UPLOAD_FOLDER)
                video_urls.append(f'/uploads/{vname}')
            except Exception as e:
                print(f"Skip invalid video {getattr(video, 'filename', '?')} during update: {e}")

        photo_urls_json = json.dumps(photo_urls)
        video_urls_json = json.dumps(video_urls)

        try:
            update_query = text("""
                UPDATE posts 
                SET Title = :title, content = :content, ProductName = :product_name, 
                    CategoryID = :category_id, photo_url = :photo_urls, video_url = :video_urls, 
                    updated_at = NOW()
                WHERE id = :post_id AND user_id = :user_id
            """)
            result = db.session.execute(update_query, {
                'post_id': id,
                'user_id': user_id,
                'title': Title,
                'content': content,
                'product_name': ProductName,
                'category_id': CategoryID,
                'photo_urls': photo_urls_json,
                'video_urls': video_urls_json
            })
            db.session.commit()

            if result.rowcount == 0:
                return jsonify({"error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï"}), 404

            print(f"Post updated successfully with ID: {id}")
            return jsonify({
                "post_id": id,
                "Title": Title,
                "content": content,
                "ProductName": ProductName,
                "CategoryID": CategoryID,
                "video_urls": video_urls,
                "photo_urls": photo_urls
            }), 200

        except Exception as db_error:
            print(f"Database error: {db_error}")
            db.session.rollback()
            # ‡πÑ‡∏°‡πà‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ
            new_files = [p for p in photo_urls if p not in (existing_photos or [])]
            for url in new_files:
                try:
                    path = os.path.join(UPLOAD_FOLDER, os.path.basename(url))
                    if os.path.exists(path):
                        os.remove(path)
                except Exception:
                    pass
            new_videos = [v for v in video_urls if v not in (existing_videos or [])]
            for url in new_videos:
                try:
                    path = os.path.join(UPLOAD_FOLDER, os.path.basename(url))
                    if os.path.exists(path):
                        os.remove(path)
                except Exception:
                    pass
            return jsonify({"error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ"}), 500

    except Exception as error:
        print("Internal server error:", str(error))
        return jsonify({"error": "Internal server error"}), 500


# === Update user profile: ‡∏ï‡∏£‡∏ß‡∏à‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö, ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö, ‡∏ï‡∏£‡∏ß‡∏à NSFW ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå ===
@app.route("/ai/users/<int:userId>/profile", methods=['PUT'])
def update_user_profile(userId):
    try:
        username = request.form.get('username')
        bio = request.form.get('bio')
        gender = request.form.get('gender')
        birthday_str = request.form.get('birthday')
        profile_image_file = request.files.get('profileImage')

        if not all([username, bio, gender, birthday_str]):
            return jsonify({
                "error": "Please fill out all required fields: Username, Bio, Gender, and Date of Birth."
            }), 400

        # parse ‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
        birthday = None
        date_formats = ["%Y-%m-%d", "%d/%m/%Y", "%m/%d/%Y", "%Y/%m/%d", "%d-%m-%Y"]
        try:
            birthday = datetime.fromisoformat(birthday_str).strftime("%Y-%m-%d")
        except ValueError:
            for fmt in date_formats:
                try:
                    birthday = datetime.strptime(birthday_str, fmt).strftime("%Y-%m-%d")
                    break
                except ValueError:
                    continue
        if birthday is None:
            return jsonify({"error": "Invalid date of birth format. Please use the format: **YYYY-MM-DD**. "}), 400

        profile_image_path = None
        if profile_image_file and profile_image_file.filename:
            temp_image_path = None
            try:
                fname, temp_image_path = _save_upload(profile_image_file, ALLOWED_IMAGE_EXTS, UPLOAD_FOLDER)
                print(f"Processing new profile image: {fname}")
                is_nude, result = nude_predict_image(temp_image_path)
                if is_nude:
                    print(f"NSFW detected in profile image {fname}")
                    if os.path.exists(temp_image_path):
                        os.remove(temp_image_path)
                    return jsonify({
                        "status": "warning",
                        "message": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
                        "details": result
                    }), 400
                else:
                    print(f"Profile image {fname} is safe")
                    profile_image_path = f'/uploads/{fname}'
            except Exception as e:
                print(f"Error processing profile image {getattr(profile_image_file, 'filename', '?')}: {e}")
                if temp_image_path and os.path.exists(temp_image_path):
                    os.remove(temp_image_path)
                return jsonify({
                    "error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ",
                    "details": str(e)
                }), 500

        # ‡∏ï‡∏£‡∏ß‡∏à username ‡∏ã‡πâ‡∏≥ ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á
        check_username_query = text("SELECT id FROM users WHERE username = :username AND id != :user_id")
        check_results = db.session.execute(check_username_query, {
            'username': username,
            'user_id': userId
        }).fetchall()
        if len(check_results) > 0:
            return jsonify({"error": "‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ô‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß"}), 400

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á query ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
        update_profile_query = "UPDATE users SET username = :username, bio = :bio, gender = :gender, birthday = :birthday"
        update_data = {'username': username, 'bio': bio, 'gender': gender, 'birthday': birthday, 'user_id': userId}
        if profile_image_path:
            update_profile_query += ", picture = :picture"
            update_data['picture'] = profile_image_path
        update_profile_query += " WHERE id = :user_id"

        try:
            result = db.session.execute(text(update_profile_query), update_data)
            db.session.commit()

            if result.rowcount == 0:
                return jsonify({"error": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"}), 404

            return jsonify({
                "message": "‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à",
                "profileImage": profile_image_path if profile_image_path else "No new image uploaded",
                "username": username,
                "bio": bio,
                "gender": gender,
                "birthday": birthday
            }), 200

        except Exception as db_error:
            print(f"Database error during profile update: {db_error}")
            db.session.rollback()
            # ‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏£‡∏π‡∏õ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡∏π‡πà ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏ã‡∏ü
            if profile_image_path:
                try:
                    p = os.path.join(UPLOAD_FOLDER, os.path.basename(profile_image_path))
                    if os.path.exists(p):
                        os.remove(p)
                except Exception:
                    pass
            return jsonify({"error": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ì‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ"}), 500

    except Exception as error:
        print(f"Internal server error: {error}")
        return jsonify({"error": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå"}), 500


# ==================== PromptPay: Generate QR ====================
@app.route('/api/generate-qrcode/<int:order_id>', methods=['GET'])
def api_generate_qrcode(order_id):
    """
    API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á PromptPay QR Code ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠.
    ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á QR Code ‡πÑ‡∏î‡πâ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà 'approved'
    ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏ï‡πà‡∏≠‡∏≠‡∏≤‡∏¢‡∏∏‡∏ó‡∏µ‡πà 'pending'.
    """
    result = generate_promptpay_qr_for_order(order_id)
    if not result['success']:
        return jsonify(result), 400
    return jsonify({
        'success': True,
        'order_id': order_id,
        'qrcode_base64': result.get('qrcode_base64'),
        'promptpay_payload': result.get('payload')
    })


# ==================== PromptPay: Verify Slip ====================
@app.route('/api/verify-slip/<int:order_id>', methods=['POST'])
def api_verify_slip(order_id):
    """Receive slip + payload, call SlipOK, update order/ad, return short EN messages."""
    file = request.files.get('slip_image')
    if not file or file.filename == '':
        return jsonify({'success': False, 'message': 'No slip received. Attach a slip image.'}), 400

    # ‡πÄ‡∏ä‡πá‡∏Å‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡πÄ‡∏ö‡∏≤‡πÜ ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏õ‡πÑ‡∏Å‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏•‡πâ‡∏°
    if not _ext_ok(file.filename, ALLOWED_IMAGE_EXTS):
        return jsonify({'success': False, 'message': 'Unsupported slip file type.'}), 400

    payload = request.form.get('payload')
    if not payload:
        return jsonify({'success': False, 'message': 'Missing payload (QR data).'}), 400

    order = find_order_by_id(order_id)
    if not order:
        return jsonify({'success': False, 'message': 'Order not found.'}), 404
    if not can_upload_slip(order):
        return jsonify({'success': False, 'message': 'Slip upload not allowed for this order status.'}), 400

    slip_dir = 'Slip'
    os.makedirs(slip_dir, exist_ok=True)
    unique_filename = f"{uuid.uuid4()}_{secure_filename(file.filename)}"
    save_path = os.path.join(slip_dir, unique_filename)
    file.save(save_path)

    print(f"[INFO] verify-slip: order={order_id}, file={unique_filename}, payload_len={len(payload)}")

    result = verify_payment_and_update_status(order_id, save_path, payload, db)
    status = 200 if result.get('success') else 400
    return jsonify(result), status


# ==================== Internal: Manual run expiry check (for testing) ====================
@app.route('/internal/run-expiry-check', methods=['POST'])
def internal_run_expiry_check():
    """
    Body (JSON) optional: {"now": "YYYY-MM-DD"}  -> ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mock ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    Returns: {count: n, ad_ids: [...]}
    """
    payload = request.get_json(silent=True) or {}
    now_str = payload.get('now')
    try:
        today_date = datetime.fromisoformat(now_str).date() if now_str else date.today()
    except Exception:
        return jsonify({'success': False, 'message': 'invalid date format for now. use YYYY-MM-DD'}), 400

    count, ad_ids = check_ads_expiring_soon(db, today_date)
    return jsonify({'success': True, 'count': count, 'ad_ids': ad_ids}), 200


# ==================== Scheduler bootstrap (‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°) ====================
_scheduler_started = False

def start_scheduler_once():
    global _scheduler_started
    if not _scheduler_started:
        # ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏∂‡∏á‡πÉ‡∏ä‡πâ start_expiry_scheduler(app, db) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ job ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ app_context
        start_expiry_scheduler(app, db)
        _scheduler_started = True

if __name__ == '__main__':
    # ‡∏Ñ‡∏á‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡πÄ‡∏î‡∏¥‡∏°: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢ app, db
    start_expiry_scheduler(app, db)
    app.run(host='0.0.0.0', port=5005)
